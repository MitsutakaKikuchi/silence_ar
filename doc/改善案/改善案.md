インスタレーションとしての「独創性」を最大化するため、単なる奥行き表現（Depth Map）にとどまらず、作品のコンセプトである**「解剖（Anatomy）」と「沈黙の有機的な蠢き」**を視覚化する、高度なシェーダー実装を提案します。

提案：『Anatomical Reveal Shader（解剖学的露見シェーダー）』
単にAからBへ切り替わるのではなく、**「表層（日常/Layer A）が、内圧（本音/Layer B）によって有機的に浸食され、裂け目から深淵が覗く」**表現を実装します。

独創性を高めるポイント：

視線追従パララックス: 端末の角度（視線）に応じて、Depth Mapに基づいた「穴の奥」が動いて見える（だまし絵的な立体感）。

有機的ノイズ浸食: 境界線が直線ではなく、細胞や黴（かび）のように有機的なノイズを持って変容する。

色収差（Chromatic Aberration）: 境界線にデジタルな「痛み」や「ノイズ」を表す色ズレを発生させる。

追加するソースコード
以下のコードを、index.html の <script> タグ内（既存のスクリプトの最後、</body> の直前など）に追加してください。これは A-Frame のカスタムコンポーネントとして動作します。

JavaScript
/**
 * Component: Anatomical Reveal Shader
 * 概要: Depth Mapを用いて、日常(Layer A)と本音(Layer B)を有機的に合成し、
 * 視差効果とノイズによる「解剖」演出を行う高機能シェーダー。
 */
AFRAME.registerComponent('anatomical-reveal', {
    schema: {
        layerA: { type: 'map' },      // 日常（表層）画像
        layerB: { type: 'map' },      // 本音（深層）画像
        depthMap: { type: 'map' },    // 深度マップ（白=手前, 黒=奥）
        strength: { type: 'number', default: 0.15 }, // 視差の強さ
        edgeColor: { type: 'color', default: '#00ffaa' } // 境界線の発光色
    },

    init: function () {
        const data = this.data;
        const el = this.el;

        // シェーダーマテリアルの定義
        this.material = new THREE.ShaderMaterial({
            uniforms: {
                uTexA: { value: null },      // Layer A
                uTexB: { value: null },      // Layer B
                uDepth: { value: null },     // Depth Map
                uTime: { value: 0 },         // 時間経過
                uParallax: { value: data.strength },
                uViewVec: { value: new THREE.Vector3(0, 0, 1) }, // 視線ベクトル
                uReveal: { value: 0.0 },     // 露見度 (0.0=日常, 1.0=本音)
                uEdgeColor: { value: new THREE.Color(data.edgeColor) }
            },
            vertexShader: `
                varying vec2 vUv;
                varying vec3 vViewPosition;
                varying vec3 vNormal;

                void main() {
                    vUv = uv;
                    vNormal = normalize(normalMatrix * normal);
                    vec4 mvPosition = modelViewMatrix * vec4(position, 1.0);
                    vViewPosition = -mvPosition.xyz;
                    gl_Position = projectionMatrix * mvPosition;
                }
            `,
            fragmentShader: `
                uniform sampler2D uTexA;
                uniform sampler2D uTexB;
                uniform sampler2D uDepth;
                uniform float uTime;
                uniform float uParallax;
                uniform float uReveal; // 0.0 ~ 1.0
                uniform vec3 uEdgeColor;

                varying vec2 vUv;
                varying vec3 vViewPosition;

                // --- Noise Functions (Simplex Noise) ---
                vec3 permute(vec3 x) { return mod(((x*34.0)+1.0)*x, 289.0); }
                float snoise(vec2 v){
                    const vec4 C = vec4(0.211324865405187, 0.366025403784439,
                            -0.577350269189626, 0.024390243902439);
                    vec2 i  = floor(v + dot(v, C.yy) );
                    vec2 x0 = v -   i + dot(i, C.xx);
                    vec2 i1;
                    i1 = (x0.x > x0.y) ? vec2(1.0, 0.0) : vec2(0.0, 1.0);
                    vec4 x12 = x0.xyxy + C.xxzz;
                    x12.xy -= i1;
                    i = mod(i, 289.0);
                    vec3 p = permute( permute( i.y + vec3(0.0, i1.y, 1.0 ))
                    + i.x + vec3(0.0, i1.x, 1.0 ));
                    vec3 m = max(0.5 - vec3(dot(x0,x0), dot(x12.xy,x12.xy), dot(x12.zw,x12.zw)), 0.0);
                    m = m*m ;
                    m = m*m ;
                    vec3 x = 2.0 * fract(p * C.www) - 1.0;
                    vec3 h = abs(x) - 0.5;
                    vec3 ox = floor(x + 0.5);
                    vec3 a0 = x - ox;
                    m *= 1.79284291400159 - 0.85373472095314 * ( a0*a0 + h*h );
                    vec3 g;
                    g.x  = a0.x  * x0.x  + h.x  * x0.y;
                    g.yz = a0.yz * x12.xz + h.yz * x12.yw;
                    return 130.0 * dot(m, g);
                }

                void main() {
                    // 1. 視差マッピング (Parallax Mapping)
                    // 視線ベクトルからUVのずらし量を計算
                    vec3 viewDir = normalize(vViewPosition);
                    float depthVal = texture2D(uDepth, vUv).r;
                    
                    // 簡易視差: 深い部分(黒)ほど視点移動の影響を受ける
                    vec2 parallaxOffset = viewDir.xy * (depthVal * uParallax * -0.1); 
                    vec2 uvB = vUv + parallaxOffset; // Layer B（奥）はずらす
                    vec2 uvA = vUv;                  // Layer A（手前）はそのまま

                    // 2. 有機的トランジション (Organic Noise Reveal)
                    // 時間経過とノイズを使って、境界をぐにゃぐにゃさせる
                    float noiseVal = snoise(vUv * 3.0 + vec2(0, uTime * 0.1));
                    float mask = uReveal + (noiseVal * 0.1); 
                    
                    // 深度マップ自体もマスクの一部に使う（深いところから裂ける演出）
                    float depthFactor = 1.0 - depthVal; // 0=奥, 1=手前
                    mask = smoothstep(0.4, 0.6, mask * 1.2 - depthFactor * 0.2);

                    // 3. テクスチャ取得 & 色収差 (Chromatic Aberration)
                    vec4 colorA = texture2D(uTexA, uvA);
                    
                    // 境界付近で色ズレを起こす
                    float aber = 0.01 * uParallax;
                    float r = texture2D(uTexB, uvB + vec2(aber, 0.0)).r;
                    float g = texture2D(uTexB, uvB).g;
                    float b = texture2D(uTexB, uvB - vec2(aber, 0.0)).b;
                    vec4 colorB = vec4(r, g, b, 1.0);

                    // 4. 合成とエッジ発光
                    // 境界線(edge)を検出
                    float edge = smoothstep(0.0, 0.1, mask) * (1.0 - smoothstep(0.9, 1.0, mask));
                    vec3 edgeEmission = uEdgeColor * edge * 2.0; // 強く発光

                    // AとBを混ぜる
                    vec4 finalColor = mix(colorA, colorB, mask);
                    finalColor.rgb += edgeEmission;

                    gl_FragColor = finalColor;
                }
            `,
            transparent: true,
            side: THREE.DoubleSide
        });

        // テクスチャローダー
        const loader = new THREE.TextureLoader();
        const loadTex = (paramName, uniformName) => {
            if (data[paramName]) {
                loader.load(data[paramName], (tex) => {
                    this.material.uniforms[uniformName].value = tex;
                });
            }
        };

        loadTex('layerA', 'uTexA');
        loadTex('layerB', 'uTexB');
        loadTex('depthMap', 'uDepth');

        // メッシュへの適用
        const mesh = el.getObject3D('mesh');
        if (mesh) {
            mesh.material = this.material;
        } else {
            const geo = new THREE.PlaneGeometry(1, 1, 32, 32); // 頂点多めで
            const newMesh = new THREE.Mesh(geo, this.material);
            el.setObject3D('mesh', newMesh);
        }
    },

    tick: function (time, timeDelta) {
        if (!this.material) return;

        // 時間更新 (ゆっくりとノイズを動かす)
        this.material.uniforms.uTime.value = time / 1000;

        // 【インタラクション制御】
        // カメラとターゲットの角度を取得し、正面から見たときだけ「本音」が見える、
        // あるいは逆に「視線を逸らすと見える」などの演出が可能。
        // ここでは、現在の「カードUI」が表示されたら uReveal を上げる想定。
        
        // カードが表示されているかチェック（index.htmlのclassを監視）
        const card = document.querySelector('.card');
        const isVisible = card && card.classList.contains('visible');
        
        // 目標値
        const targetReveal = isVisible ? 1.0 : 0.0;
        
        // 現在値から目標値へ滑らかに補間 (Lerp)
        const current = this.material.uniforms.uReveal.value;
        this.material.uniforms.uReveal.value += (targetReveal - current) * 0.05;

        // ジャイロ/カメラ視差の反映
        // カメラのワールド座標を取得してシェーダーに渡す（簡易実装）
        const cam = this.el.sceneEl.camera;
        if(cam) {
            // カメラの方向ベクトル等を用いて計算可能だが、
            // A-Frame(MindAR)ではカメラが原点固定でWorldが動くため、
            // ターゲットのワールド回転から相対的な視線ベクトルを逆算する。
            const targetQuat = new THREE.Quaternion();
            this.el.object3D.getWorldQuaternion(targetQuat);
            
            // 簡易的にZ軸の傾きを利用
            const v = new THREE.Vector3(0,0,1);
            v.applyQuaternion(targetQuat.invert());
            this.material.uniforms.uViewVec.value = v;
        }
    }
});
HTMLへの適用方法
index.html の a-scene 内にある各ターゲットエンティティ (<a-entity mindar-image-target...) の中身を、以下のように書き換えてください。 ※ 画像パスは実際のものに合わせてください。Depth Map (_depth.jpg) は別途作成が必要です。

HTML
<a-entity mindar-image-target="targetIndex: 0" id="target-0">
    
    <a-entity 
        geometry="primitive: plane; width: 1; height: 1" 
        position="0 0 0" 
        rotation="0 0 0" 
        anatomical-reveal="
            layerA: ./images/target_source.jpg; 
            layerB: ./images/layerB_0.jpg; 
            depthMap: ./images/layerB_0_depth.jpg; 
            strength: 0.3;
            edgeColor: #00ffaa
        ">
    </a-entity>

</a-entity>
実装のポイントと演出意図
anatomical-reveal コンポーネント:

Layer A: 日常の写真を指定します（MindARのターゲット画像と同じもの）。

Layer B: 生成AIで作った「本音」の写真を指定します。

Depth Map: Photoshopのニューラルフィルターや、Web上のAIツール（Leiapix等）で Layer B から生成した深度マップ（白黒画像）を指定します。

tick 関数のロジック (重要):

現在のコードでは、.card クラスを持つ要素（既存の解説UI）が表示されているかどうか（visibleクラス）を検知して、uReveal（露見度）を 0.0 から 1.0 へ滑らかに変化させています。

演出: スキャン直後は「日常（Layer A）」ですが、ターゲットを認識して解説が出ると、AR上の画像が有機的に裂けて「本音（Layer B）」が露出し、その奥に奥行き（Depth）を感じるようになります。

シェーダーの表現:

snoise (シンプレックスノイズ) を使用し、境界線が燃え広がるように、あるいは細胞が侵食するように変化します。

境界線に uEdgeColor で指定した色（例：ミントグリーン）が発光し、コンセプトカラーである「わびさび＋ネオン」の世界観を強調します。

次のステップ（準備）
このコードを動かすためには、各エピソードに対応する Depth Map（深度画像） が必要です。

ファイル名案: layerB_0_depth.jpg, layerB_1_depth.jpg ...

作り方: お手持ちの layerB_xx.jpg を、無料の深度マップ生成AI（例: "Depth Anything" のHugging Faceデモなど）に通して、白黒画像を保存してください。

この実装により、静止画の切り替えだけだったARが、**「鑑賞者の行為（スキャン）によって、日常の皮膜が剥がれ落ちる」**という動的で哲学的な体験へと昇華されます。