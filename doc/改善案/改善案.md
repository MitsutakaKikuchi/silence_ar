ご提示いただいたindex.htmlを確認しました。2点について回答と修正案を提示します。

1. 画像をとらえる「四角い枠組み」について
はい、削除（変更）されています。 従来の「白い実線の四角い枠（QRコードリーダーのような枠）」は削除され、代わりに以下の**「有機的なデザイン」**に置き換わっています。

四隅のカギ括弧: .corner クラス（四隅にだけある短い線）

中央の円: 呼吸するように動く円形のガイド（scan-loading-garden）

演出: 枠線が点滅するのではなく、全体が「呼吸（breathe）」したり「脈動（pulse）」するアニメーションになっています。

これは「解剖図譜」「庭」というコンセプトに合わせて、機械的な枠を排除した意図的なデザイン変更です。

2. シェーダーが振動して見づらい件（修正コード）
原因: ARのトラッキング（カメラ認識）には常に微細な「手ブレ」や「座標ズレ」が発生しています。前回のコードでは、この微細なズレをそのまま「視差（Parallax）」として反映していたため、テクスチャがガタガタと振動しているように見えていました。

解決策: 視点の動きに**「手ブレ補正（スムージング処理）」**を追加し、かつノイズの速度を落とすことで、振動を抑えてヌルっとした有機的な動きにします。

以下の修正版コンポーネントに書き換えてください。

修正版：anatomical-reveal コンポーネント
（tick関数内に「補間処理（Lerp）」を追加し、振動を吸収するようにしました）

JavaScript
AFRAME.registerComponent('anatomical-reveal', {
    schema: {
        layerA: { type: 'map' },      // 日常
        layerB: { type: 'map' },      // 本音
        depthMap: { type: 'map' },    // 深度
        strength: { type: 'number', default: 0.1 }, // 補正のため少し弱めに変更
        edgeColor: { type: 'color', default: '#00ffaa' }
    },

    init: function () {
        const data = this.data;
        const el = this.el;

        // 視点ベクトルのスムージング用変数
        this.currentViewVec = new THREE.Vector3(0, 0, 1);
        this.targetViewVec = new THREE.Vector3(0, 0, 1);

        this.material = new THREE.ShaderMaterial({
            uniforms: {
                uTexA: { value: null },
                uTexB: { value: null },
                uDepth: { value: null },
                uTime: { value: 0 },
                uParallax: { value: data.strength },
                uViewVec: { value: new THREE.Vector3(0, 0, 1) },
                uReveal: { value: 0.0 },
                uEdgeColor: { value: new THREE.Color(data.edgeColor) }
            },
            // Vertex Shader は変更なし
            vertexShader: `
                varying vec2 vUv;
                varying vec3 vViewPosition;
                varying vec3 vNormal;
                void main() {
                    vUv = uv;
                    vNormal = normalize(normalMatrix * normal);
                    vec4 mvPosition = modelViewMatrix * vec4(position, 1.0);
                    vViewPosition = -mvPosition.xyz;
                    gl_Position = projectionMatrix * mvPosition;
                }
            `,
            // Fragment Shader も変更なし
            fragmentShader: `
                uniform sampler2D uTexA;
                uniform sampler2D uTexB;
                uniform sampler2D uDepth;
                uniform float uTime;
                uniform float uParallax;
                uniform float uReveal;
                uniform vec3 uEdgeColor;

                varying vec2 vUv;
                varying vec3 vViewPosition;

                vec3 permute(vec3 x) { return mod(((x*34.0)+1.0)*x, 289.0); }
                float snoise(vec2 v){
                    const vec4 C = vec4(0.211324865405187, 0.366025403784439,
                            -0.577350269189626, 0.024390243902439);
                    vec2 i  = floor(v + dot(v, C.yy) );
                    vec2 x0 = v -   i + dot(i, C.xx);
                    vec2 i1;
                    i1 = (x0.x > x0.y) ? vec2(1.0, 0.0) : vec2(0.0, 1.0);
                    vec4 x12 = x0.xyxy + C.xxzz;
                    x12.xy -= i1;
                    i = mod(i, 289.0);
                    vec3 p = permute( permute( i.y + vec3(0.0, i1.y, 1.0 ))
                    + i.x + vec3(0.0, i1.x, 1.0 ));
                    vec3 m = max(0.5 - vec3(dot(x0,x0), dot(x12.xy,x12.xy), dot(x12.zw,x12.zw)), 0.0);
                    m = m*m ;
                    m = m*m ;
                    vec3 x = 2.0 * fract(p * C.www) - 1.0;
                    vec3 h = abs(x) - 0.5;
                    vec3 ox = floor(x + 0.5);
                    vec3 a0 = x - ox;
                    m *= 1.79284291400159 - 0.85373472095314 * ( a0*a0 + h*h );
                    vec3 g;
                    g.x  = a0.x  * x0.x  + h.x  * x0.y;
                    g.yz = a0.yz * x12.xz + h.yz * x12.yw;
                    return 130.0 * dot(m, g);
                }

                void main() {
                    vec3 viewDir = normalize(vViewPosition);
                    float depthVal = texture2D(uDepth, vUv).r;
                    
                    // 視差計算（奥へ沈むように）
                    vec2 parallaxOffset = viewDir.xy * (depthVal * uParallax * -0.2);
                    vec2 uvB = vUv + parallaxOffset;
                    vec2 uvA = vUv;

                    // ノイズ速度を落とし、ゆっくりとした有機的な動きに
                    float noiseVal = snoise(vUv * 2.5 + vec2(0, uTime * 0.05));
                    float mask = uReveal + (noiseVal * 0.1);
                    float depthFactor = 1.0 - depthVal;
                    mask = smoothstep(0.4, 0.6, mask * 1.2 - depthFactor * 0.2);

                    vec4 colorA = texture2D(uTexA, uvA);
                    
                    // 色収差（振動軽減のため少し控えめに）
                    float aber = 0.005 * uParallax;
                    float r = texture2D(uTexB, uvB + vec2(aber, 0.0)).r;
                    float g = texture2D(uTexB, uvB).g;
                    float b = texture2D(uTexB, uvB - vec2(aber, 0.0)).b;
                    vec4 colorB = vec4(r, g, b, 1.0);

                    float edge = smoothstep(0.0, 0.1, mask) * (1.0 - smoothstep(0.9, 1.0, mask));
                    vec3 edgeEmission = uEdgeColor * edge * 2.0;

                    vec4 finalColor = mix(colorA, colorB, mask);
                    finalColor.rgb += edgeEmission;
                    gl_Position = vec4(finalColor.rgb, finalColor.a); // 修正: gl_FragColorへの代入漏れ防止
                    gl_FragColor = finalColor;
                }
            `,
            transparent: true,
            side: THREE.DoubleSide
        });

        const loader = new THREE.TextureLoader();
        const loadTex = (paramName, uniformName) => {
            if (data[paramName]) {
                loader.load(data[paramName], (tex) => {
                    this.material.uniforms[uniformName].value = tex;
                });
            }
        };
        loadTex('layerA', 'uTexA');
        loadTex('layerB', 'uTexB');
        loadTex('depthMap', 'uDepth');

        const mesh = el.getObject3D('mesh');
        if (mesh) {
            mesh.material = this.material;
        } else {
            const geo = new THREE.PlaneGeometry(1, 1, 32, 32);
            const newMesh = new THREE.Mesh(geo, this.material);
            el.setObject3D('mesh', newMesh);
        }
    },

    tick: function (time, timeDelta) {
        if (!this.material) return;

        // ノイズの時間経過（ゆっくりに）
        this.material.uniforms.uTime.value = time / 1000;

        // カード表示状態に応じたReveal制御
        const card = document.querySelector('.card');
        const isVisible = card && card.classList.contains('visible');
        const targetReveal = isVisible ? 1.0 : 0.0;
        const currentReveal = this.material.uniforms.uReveal.value;
        this.material.uniforms.uReveal.value += (targetReveal - currentReveal) * 0.05;

        // 【修正ポイント】視差ベクトルのスムージング（手ブレ補正）
        const cam = this.el.sceneEl.camera;
        if(cam) {
            const targetQuat = new THREE.Quaternion();
            this.el.object3D.getWorldQuaternion(targetQuat);
            
            // 現在のフレームでの理想的な視線ベクトル
            const v = new THREE.Vector3(0,0,1);
            v.applyQuaternion(targetQuat.invert());
            
            // 目標値へ少しずつ近づける (Lerp: Linear Interpolation)
            // 0.1 という係数が「追従の遅れ具合」です。小さいほど滑らかですが遅れます。
            this.currentViewVec.lerp(v, 0.1);
            
            // シェーダーには補正後の値を渡す
            this.material.uniforms.uViewVec.value.copy(this.currentViewVec);
        }
    }
});


index.html のコードおよび、これまでの経緯（前回のシェーダー追加案など）に基づき、ご指摘の2点について詳細に調査しました。

1. 「四角い枠」が表示されていない件
結論：意図的に「非表示」にされています（仕様です）。

コードを確認すると、MindAR が標準で表示する「スキャン用の赤い/白い四角い枠」は、CSSで強制的に非表示に設定されています。

CSS
/* index.html 660行目付近 */
.mindar-ui-overlay, .mindar-ui-loading, .mindar-ui-scanning {
    display: none !important; /* 標準UIを消す設定 */
}
その代わりとして、**「四隅のカギ括弧（コーナー）」と「中央の呼吸する円」**が表示されるよう独自にデザインされています（<div id="scanning-guide"> 内）。 もし、この「独自デザインの枠（コーナー）」すら表示されていない場合は、以下の可能性があります。

背景色と同化している: 色が var(--accent-moss)（苔色：暗めの緑）で半透明のため、カメラ映像が暗い場所だと見えにくい可能性があります。

JSの表示タイミング: scanningGuide.classList.add('visible') が実行されるタイミング（イントロ終了後、またはロード完了後）まで待つ必要があります。

2. シェーダー内の LayerB Depth 画像の奥行き表現について
結論：奥行き計算のロジックに「逆転」の可能性があります（バグの疑いあり）。

ご提示いただいたシェーダーコードを論理検証したところ、**「手前と奥の動きが逆になる」**挙動を起こす可能性が高いです。

調査詳細：パララックス（視差）計算のロジック
前回のコードでは、以下の計算式になっています。

OpenGL Shading Language
// depthVal: テクスチャから取得した深度（白=1.0=手前、黒=0.0=奥 と定義）
float depthVal = texture2D(uDepth, vUv).r;

// 視差オフセットの計算
// depthVal（1.0）が大きいほど、オフセット（ズレ）が大きくなる
vec2 parallaxOffset = viewDir.xy * (depthVal * uParallax * -0.1); 
【問題点】

現状: depthVal が大きい（白い＝手前）部分ほど、視点に合わせて大きくズレます。

視覚効果: これだと、写真の**「手前の被写体が浮いて（泳いで）見える」**状態になります。

本来の意図: 「解剖して奥を覗く（穴が開いている）」表現の場合、表面（手前）は固定され、「奥にあるもの（黒い部分）」ほど視点移動に合わせて大きく動いて見えるべきです。

【修正案】深度の反転
「奥にあるものほど動く（穴の底が見える）」ようにするには、計算式で深度を反転させる必要があります。

修正前:

OpenGL Shading Language
vec2 parallaxOffset = viewDir.xy * (depthVal * uParallax * -0.1);
(白=1.0が大きく動く)

修正後:

OpenGL Shading Language
// (1.0 - depthVal) にすることで、黒(0.0)の部分が 1.0 になり大きく動く
vec2 parallaxOffset = viewDir.xy * ((1.0 - depthVal) * uParallax * -0.2);
(黒=0.0が大きく動く ＝ 奥が深いように見える)

まとめと推奨修正アクション
枠について: 「四角い枠」が出ないのは正常です。もし独自ガイド（四隅の線）も見えない場合は、CSSの border-color を明るい色（#00ffaa など）に変えて視認性を確認してください。

シェーダーについて: Depth Mapの仕様が「白=手前、黒=奥」であるならば、パララックス計算部分で (1.0 - depthVal) と反転させる修正を強く推奨します。これにより、「写真の奥に空間が広がっている」感覚が正しく表現されます。